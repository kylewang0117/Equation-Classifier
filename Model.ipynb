{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model.ipynb","provenance":[],"authorship_tag":"ABX9TyOSKaaRnBim7MOe4cwz20Um"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Group 17, Primary Model and Architecture\n","Group members: Tiger Luo, Kyle Wang\n","\n","APS360\n","\n","August 15, 2022"],"metadata":{"id":"ZsHAKzDslPh0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaEdwfc1lJa2"},"outputs":[],"source":["#Libraries used\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torchvision\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from google.colab import drive\n","from PIL import Image, ImageOps\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Autoencoder"],"metadata":{"id":"N_AbpmCwlgod"}},{"cell_type":"code","source":["#This is what our autoencoder looks like\n","class HMEAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(HMEAutoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 24, 3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(24, 48, 3, stride=2, padding=1)\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(48, 24, 3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(24, 3, 3, stride=2, padding=1, output_padding=1),\n","            nn.Sigmoid()\n","        )\n","      \n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"2vWIU6S7lkRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([transforms.PILToTensor()])\n","\n","local_dir = \"\" #Change to the directory of folder the project folder is in\n","\n","file_dir = local_dir + \"/Group 17 - Final Deliverable/Autoencoder data\" \n","\n","data=torchvision.datasets.ImageFolder(file_dir)"],"metadata":{"id":"sqY5xj2qlr_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_a_data=[]\n","for i in range(4000): #Can change 4000 to anything needed\n","    train_a_data.append(data[i][0])"],"metadata":{"id":"9Iumz4iCl-1i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This is our primary training block, all the functions used in training the autoencoder are here\n","\n","def rand_invert(image): #Adds a 50% chance to invert image colours\n","    rand=torch.rand(1)\n","    if rand[0]<0.5:\n","        image=transforms.functional.invert(image)\n","        #print(\"inverted\") #For debugging purposes\n","    #else: \n","        #print(\"not inverted\")\n","    return image\n","\n","def image_processing(image):\n","    image=transform(image)/255 #Converts from int to float type pictures\n","    height=image.shape[1]\n","    width=image.shape[2]\n","    height=4*int(height/4) #rounds height and width down to the nearest multiple of 4\n","    width=4*int(width/4)\n","    image=image[:,:height,:width]\n","\n","    augmented_image=augment(image) #50% chance to invert\n","    return image, augmented_image #Returns cropped and augmented images\n","\n","def augment(image): #Put image augmentations here, perhaps could've done other augmentations, but there's only two for now\n","    #image=image-0.4*torch.randn(*image.shape) #Adding noise, doesn't do much so we removed it for now\n","    image=np.clip(image,0.,1.)\n","    image=rand_invert(image)\n","    return image\n","\n","def train(model, HME_Data, num_epochs=100, learning_rate=0.01):\n","    torch.manual_seed(20)\n","    criterion = nn.MSELoss()\n","    \n","    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","\n","    outputs = []\n","    for epoch in range(num_epochs):\n","        for data in HME_Data:\n","            optimizer.zero_grad()\n","            image, augmented_image=image_processing(data)\n","            recon = model(augmented_image)\n","            loss = criterion(recon, image)\n","            loss.backward()\n","            optimizer.step()\n","            \n","        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n","        outputs.append((epoch, loss, image, recon),)\n","    return outputs"],"metadata":{"id":"_R1ng78fmLc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["autoencoder=HMEAutoencoder()\n","outputs_auto=train(autoencoder, train_a_data, 100, 0.01) #Can do more than 100 epochs, but we only did 100 for time's sake"],"metadata":{"id":"Iigb2W96m2BX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Autoencoder Visualizations"],"metadata":{"id":"ZMrdSruhm5nH"}},{"cell_type":"code","source":["#This helps visualize how well it reconstructs over time\n","\n","fig=plt.figure(figsize=(50, 150))\n","rows=100 #Change to however many epochs you had\n","cols=1\n","\n","outputs=outputs_auto\n","\n","for epoch, loss, image, recon in outputs:\n","    recon_out=recon.detach().numpy()\n","    recon_out=np.transpose(recon_out,[1,2,0])\n","    fig.add_subplot(rows,cols,epoch+1)\n","    plt.imshow(recon_out)\n","    plt.axis('off')\n","    plt.title(\"Epoch: {}\".format(epoch))"],"metadata":{"id":"asWmsfN8m78q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This is meant to plot the loss curve\n","\n","x_axis=[]\n","y_axis=[]\n","\n","for epoch, loss, image, recon in outputs:\n","    x_axis.append(epoch)\n","    y_axis.append(loss.detach().numpy())\n","\n","plt.title(\"Loss curve\")\n","plt.plot(x_axis,y_axis, label=\"Loss\")"],"metadata":{"id":"xaTooXSenEom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Allows you to compare side by side the image and its reconstruction\n","\n","image_index=400\n","model=autoencoder #If using a different name, put it here\n","\n","fig=plt.figure(figsize=(10,10))\n","rows=2\n","cols=1\n","\n","image=image_processing(train_a_data[image_index])[1]\n","image_show=image.detach().numpy()\n","image_show=np.transpose(image_show,[1,2,0])\n","fig.add_subplot(rows,cols,1)\n","plt.imshow(image_show)\n","plt.axis('off')\n","plt.title(\"Image\")\n","\n","\n","recon=model(image)\n","recon_out=recon.detach().numpy()\n","recon_out=np.transpose(recon_out,[1,2,0])\n","fig.add_subplot(rows,cols,2)\n","plt.imshow(recon_out)\n","plt.axis('off')\n","plt.title(\"Recon\")"],"metadata":{"id":"2-2rlHJynU5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Classifier"],"metadata":{"id":"s_gThZ93neuk"}},{"cell_type":"code","source":["class HMEClassifier(nn.Module):\n","    def __init__(self):\n","        super(HMEClassifier, self).__init__()\n","        self.layer1 = nn.Linear(65*22*48, 2000)\n","        self.layer2 = nn.Linear(2000, 100)\n","        self.layer3 = nn.Linear(100, 9)\n","    def forward(self, img):\n","        flattened = img.view(-1, 65*22*48)\n","        activation1 = F.relu(self.layer1(flattened))\n","        activation2 = F.relu(self.layer2(activation1))\n","        output = self.layer3(activation2)\n","        return output\n","\n","def full_model(model, encoder, img): #The full model is the encoder, a pooling layer, and the actual classifier\n","    pool=nn.MaxPool2d(2,2)\n","\n","    encodings=encoder(img) #Converts the image to embedding\n","    encodings=pool(encodings) #Pools that embedding\n","    out=model(encodings.detach()) #Then passes it into the classifier\n","    return out"],"metadata":{"id":"oDnLV7txnePH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_image(img):\n","    img=transform(img)/255\n","    return img\n","\n","def get_accuracy(data, model, encoder):\n","    correct = 0\n","    total = 0\n","    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=100):\n","        output = full_model(model, encoder, imgs) #Puts the image through the model, one at a time, gets an output of a size 10 vector\n","        pred = output.max(1, keepdim=True)[1] \n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","    return correct / total\n","\n","\n","def train_classifier(model, encoder, train_data, val_data, epochs=80, learning_rate=0.0001): #Keep lr at 0.0001 or lower\n","    criterion=nn.CrossEntropyLoss()\n","    optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","\n","    outputs=[]\n","    \n","    train_batch=torch.utils.data.DataLoader(train_data, 40)\n","\n","    one_hot_encoding=torch.eye(9) #For turning labels into one hot encodings\n","\n","    for epoch in range(epochs):\n","        batch_no=0\n","        total_loss=0\n","        for imgs, label in train_batch:\n","            optimizer.zero_grad()\n","            label=one_hot_encoding[label] #Converts label to OHE\n","\n","            out=full_model(model, encoder, imgs)\n","            loss=criterion(out, label)\n","            #print(loss) #For debugging, previously had issue w/ exploding gradients, so default lr was lowered to 0.0001\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss=total_loss+loss #For averaging loss\n","            batch_no+=1\n","\n","        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(total_loss/batch_no))) #Prints average loss\n","        \n","        train_acc=get_accuracy(train_data, model, encoder)\n","        val_acc=get_accuracy(val_data, model, encoder)\n","        print('Train acc: {:.4f}, val acc: {:.4f}'.format(train_acc, val_acc))\n","        outputs.append((epoch, total_loss/batch_no, train_acc, val_acc),)\n","\n","    return outputs"],"metadata":{"id":"n9DGomcZnv6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_dir=local_dir + \"/Group 17 - Final Deliverable/Classified HMEs\" \n","classifier_data=torchvision.datasets.ImageFolder(classifier_dir)\n","\n","print(classifier_data) #Total amount of classifier data is 920 entries"],"metadata":{"id":"CUhSrgdWohZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def manual_pull(iterable, indices):\n","    output=[]\n","    for i in indices:\n","        image=transform(iterable[i][0])/255\n","        label=iterable[i][1]\n","        output.append((image,label))\n","    return output\n","\n","np.random.seed(60)\n","data_len=len(classifier_data)\n","indices=list(range(data_len))\n","np.random.shuffle(indices)\n","train_indices=indices[:840] #Manually split the data here, you can change it if you'd like\n","val_indices=indices[840:920] \n","#No testing data, because demo data is used instead\n","\n","train_c_set=manual_pull(classifier_data,train_indices) #This is to format the data correctly\n","val_c_set=manual_pull(classifier_data,val_indices)"],"metadata":{"id":"N4ru3BS2ox3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder=autoencoder.encoder #Takes the encoder from the autoencoder\n","classifier=HMEClassifier()"],"metadata":{"id":"cwaC15Mro706"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs=train_classifier(classifier,encoder,train_c_set, val_c_set, 80, 0.0001) #Trains on 80 epochs"],"metadata":{"id":"pW0xvkcHpDkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For early stopping with patience, monitor the accuracies to ensure it doesn't overfit or go off weirdly\n","n=1 #Can go epoch by epoch, or larger steps if wanted. We recommend sticking with just 1 though\n","#We used a period of 5 or 6 epochs for early stopping\n","epochs=80 #Keep track of your epochs here, update every time you run the code\n","\n","out_temp=train_classifier(classifier,encoder,train_c_set, val_c_set, n, 0.0001)\n","if n=1:\n","    outputs.append((epochs+1,out_temp[1], out_temp[2],out_temp[3]))\n","else:\n","    count=0\n","    for out in out_temp:\n","        count+=1\n","        outputs.append((epochs+count,out[1], out[2],out[3]))"],"metadata":{"id":"-vEFnTrxpOtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Classifier Visualizations"],"metadata":{"id":"wvZltzGMpiQm"}},{"cell_type":"code","source":["#Visualizes loss curve\n","x_axis=[]\n","y_axis=[]\n","train_accs=[]\n","val_accs=[]\n","for epoch, loss, train_acc, val_acc in outputs:\n","    x_axis.append(epoch)\n","    y_axis.append(loss.detach().numpy())\n","    train_accs.append(train_acc)\n","    val_accs.append(val_acc)\n","print(outputs)\n","plt.title(\"Loss curve\")\n","plt.plot(x_axis,y_axis, label=\"Loss\")"],"metadata":{"id":"BPWPbct3pdi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizes train and val accuracies\n","plt.title(\"Training Curve\")\n","plt.plot(x_axis, train_accs, label=\"Train\")\n","plt.plot(x_axis, val_accs, label=\"Val\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Training Accuracy\")\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"_y6IWdn7pznZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For manually checking the output of each image in the classifier dataset\n","#Note that 0-99 should be class 0, 100-199 should be class 1, etc\n","\n","pool=nn.MaxPool2d(2,2)\n","for img, label in classifier_data:\n","\n","    img=process_image(img)\n","    encod=encoder(img)\n","    encod=pool(encod)\n","    encod=encod.detach()\n","    out=classifier(encod)\n","    print(\"Image no: {}, output: {:.0f}\".format(n, float(out.argmax())))\n","    n+=1"],"metadata":{"id":"jZRdfsPzp52Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For checking the accuracy class by class\n","\n","class_names=(\"ODE\", \"Complex\", \"Differentiation\", \"Inequalities\", \"Integration\", \"Limits\", \"Logarithms\", \"Parametrics\", \"Trigonometry\")\n","\n","def get_accuracy_mod(data, model, encoder):\n","    correct = 0\n","    total = 0\n","    for imgs, labels in data:\n","        output = full_model(model, encoder, imgs) #Puts the image through the model, one at a time, gets an output of a size 10 vector\n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1] \n","        if (pred==labels):\n","            correct+=1\n","        total+=1\n","    return correct / total\n","\n","def get_indices(data, classification):\n","    indices=[]\n","    for i in range(len(data)):\n","        if (data[i][1]==classification):\n","            indices.append(i)\n","    return indices\n","\n","for i in range(9):\n","    class_dataset=manual_pull(classifier_data, get_indices(classifier_data, i))\n","    accuracy=get_accuracy_mod(class_dataset, classifier, encoder)\n","    print(\"Class: {}. Accuracy: {:.4f}\".format(class_names[i], accuracy))"],"metadata":{"id":"vYnn44X8qCmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#For checking the probability distribution for an individual image\n","\n","n=200 #Put the index of the image you want to check here\n","pool=nn.MaxPool2d(2,2)\n","test_image=process_image(classifier_data[n][0])\n","encod=encoder(test_image)\n","encod=pool(encod)\n","encod=encod.detach()\n","\n","\n","image_show=test_image.detach().numpy()\n","image_show=np.transpose(image_show,[1,2,0])\n","fig.add_subplot(rows,cols,1)\n","plt.imshow(image_show)\n","plt.axis('off')\n","plt.title(\"Image\")\n","\n","output=classifier(encod)\n","prob=F.softmax(output, dim=-1)\n","prob=F.softmax(prob, dim=-1)\n","for i in range(9):\n","    print(\"{} probability: {:.4f}\".format(class_names[i], prob[0][i]*100))"],"metadata":{"id":"AEigPGoAqQgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This is just a repeat of the autoencoder reconstruction visualization, but for classifier data instead\n","\n","n=200 #Put the index of the image you want to check here\n","fig=plt.figure(figsize=(10,10))\n","rows=2\n","cols=1\n","\n","model=autoencoder\n","\n","image=image_processing(classifier_data[n][0])[1]\n","image_show=image.detach().numpy()\n","image_show=np.transpose(image_show,[1,2,0])\n","fig.add_subplot(rows,cols,1)\n","plt.imshow(image_show)\n","plt.axis('off')\n","plt.title(\"Image\")\n","\n","recon=model(image)\n","recon_out=recon.detach().numpy()\n","recon_out=np.transpose(recon_out,[1,2,0])\n","fig.add_subplot(rows,cols,2)\n","plt.imshow(recon_out)\n","plt.axis('off')\n","plt.title(\"Recon\")"],"metadata":{"id":"0L1PRDCnqebf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Demonstration"],"metadata":{"id":"3MW3LDEbqsYD"}},{"cell_type":"code","source":["demo_dir=local_dir + \"/Group 17 - Final Deliverable/Demonstration images\"\n","demo_im=torchvision.datasets.ImageFolder(demo_dir)\n","\n","demo_data=[]\n","for img, label in demo_im:\n","    img=process_image(img)\n","    demo_data.append(img)\n","print(demo_im)"],"metadata":{"id":"3MpPo-RIqtaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Repeat of checking probability distributions, but for the demonstration data\n","\n","n=61\n","pool=nn.MaxPool2d(2,2)\n","test_image=demo_data[n]\n","encod=encoder(test_image)\n","encod=pool(encod)\n","encod=encod.detach()\n","\n","\n","image_show=test_image.detach().numpy()\n","image_show=np.transpose(image_show,[1,2,0])\n","fig.add_subplot(rows,cols,1)\n","plt.imshow(image_show)\n","plt.axis('off')\n","plt.title(\"Image\")\n","\n","output=classifier(encod)\n","prob=F.softmax(output, dim=-1)\n","prob=F.softmax(prob, dim=-1)\n","for i in range(9):\n","    print(\"{} probability: {:.4f}\".format(class_names[i], prob[0][i]*100))"],"metadata":{"id":"oIKN1J4Zq2d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Repeat of visualizing autoencoder reconstruction, but for the demonstration data\n","\n","n=51\n","fig=plt.figure(figsize=(10,10))\n","rows=2\n","cols=1\n","\n","image=demo_data[n]\n","image_show=image.detach().numpy()\n","image_show=np.transpose(image_show,[1,2,0])\n","fig.add_subplot(rows,cols,1)\n","plt.imshow(image_show)\n","plt.axis('off')\n","plt.title(\"Image\")\n","\n","recon=model(image)\n","recon_out=recon.detach().numpy()\n","recon_out=np.transpose(recon_out,[1,2,0])\n","fig.add_subplot(rows,cols,2)\n","plt.imshow(recon_out)\n","plt.axis('off')\n","plt.title(\"Recon\")"],"metadata":{"id":"DUEeXTMGq9od"},"execution_count":null,"outputs":[]}]}